{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "SkipGram.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "vdUehOjo00AQ"
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Azrb2RZfXd5w"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qIV70wWh0526"
   },
   "source": [
    "def random_batch(skip_grams):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False)\n",
    "\n",
    "    for i in random_index:\n",
    "        random_inputs.append(skip_grams[i][0])  # target\n",
    "        random_labels.append(skip_grams[i][1])  # context word\n",
    "\n",
    "    return random_inputs, random_labels"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FuoAoeyA4Vvr"
   },
   "source": [
    "sentences = \"\"\"During my second month of nursing school, our professor gave us a pop quiz.  \n",
    "I was a conscientious student and had breezed through the questions, until I read the last one: \n",
    "“What is the first name of the woman who cleans the school?”  Surely this was some kind of joke.\"\"\".split()"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PWl8JRPy4KQg"
   },
   "source": [
    "batch_size = 2 # mini-batch size\n",
    "embedding_size = 10 # embedding size\n",
    "\n",
    "word_sequence = \" \".join(sentences).split()\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "\n",
    "voc_size = len(word_list)\n",
    "\n",
    "def skipgram(sentences, window_size=1):\n",
    "    skip_grams = []\n",
    "    for i in range(window_size, len(word_sequence) - window_size):\n",
    "        target = word_sequence[i]\n",
    "        context = [word_sequence[i - window_size], word_sequence[i + window_size]]\n",
    "        for w in context:\n",
    "            skip_grams.append([target, w])\n",
    "\n",
    "    return skip_grams"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_VIXjkfL4R3v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ed7ee267-267b-47b2-c38d-5631ee7340ce"
   },
   "source": [
    "skipgram(word_sequence)[0:2]"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[['my', 'During'], ['my', 'second']]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WiovprKK5T5_"
   },
   "source": [
    "class skipgramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(skipgramModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(voc_size, embedding_size)\n",
    "        self.W = nn.Linear(embedding_size, embedding_size, bias=False) \n",
    "        self.WT = nn.Linear(embedding_size, voc_size, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeddings = self.embedding(X)\n",
    "        hidden_layer = nn.functional.relu(self.W(embeddings)) \n",
    "        output_layer = self.WT(hidden_layer)\n",
    "        return output_layer\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_dict[word]])\n",
    "        return self.embedding(word).view(1,-1)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n_I8SgqQ7iQ6"
   },
   "source": [
    "model = skipgramModel()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WaOQb0f7i39",
    "outputId": "3765c217-e174-4a88-eb33-ea5aae0976ff"
   },
   "source": [
    "for epoch in tqdm(range(150000), total=len(skipgram(word_sequence))):\n",
    "    input_batch, target_batch = random_batch(skipgram(word_sequence))\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch)\n",
    "\n",
    "    # output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 10000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/96 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12984/2191470289.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m150000\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mskipgram\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[0minput_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_batch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrandom_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mskipgram\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0minput_batch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLongTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_batch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m     \u001B[0mtarget_batch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLongTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtarget_batch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: too many dimensions 'str'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2elp_bzW8B0M"
   },
   "source": [
    "def Skipgram_test(test_data, model):\n",
    "    correct_ct = 0\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        input_batch, target_batch = random_batch(test_data)\n",
    "        input_batch = torch.LongTensor(input_batch)\n",
    "        target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "        model.zero_grad()\n",
    "        _, predicted = torch.max(model(input_batch), 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if predicted[0] == target_batch[0]:\n",
    "                correct_ct += 1\n",
    "\n",
    "    print('Accuracy: {:.1f}% ({:d}/{:d})'.format(correct_ct/len(test_data)*100, correct_ct, len(test_data)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOV3AC018Squ",
    "outputId": "1b9797d9-7163-4226-c91a-7417f2a56c58"
   },
   "source": [
    "Skipgram_test(skipgram(word_sequence), model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "1HYIy_AM8gRj",
    "outputId": "866d1852-70cf-4298-93cb-7369b40e6c5d"
   },
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for w in word_list:\n",
    "    x = model.get_word_emdedding(w).detach().data.numpy()[0][0]\n",
    "    y = model.get_word_emdedding(w).detach().data.numpy()[0][1]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(w, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a5G2HLktB_1k"
   },
   "source": [
    "pred = \"Surely\".split()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KvbtE_tJDmkB"
   },
   "source": [
    "word_dict_inverse = {i:w for w, i in word_dict.items()}\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "f865TE5WEUS6",
    "outputId": "005ea604-8ab2-4207-c20c-5b6b937c6a27"
   },
   "source": [
    "model_pred = []\n",
    "e = 0\n",
    "model_pred.append(pred[0])\n",
    "\n",
    "while e<6:\n",
    "    word = word_dict_inverse[torch.argmax(model(torch.LongTensor([word_dict[model_pred[-1]]]))).item()]\n",
    "    model_pred.append(word)\n",
    "    e+=1\n",
    "\n",
    "' '.join(model_pred)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "60YfNstTQCQr"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}